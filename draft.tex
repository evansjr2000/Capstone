%
\input myindexing
\input mydefs
\input mymacros
\input manmac

%

\input twelvepoint
\twelvepoint

\ll{\bf Infra Optimization.}

\ms

Project 1

\ll{\bf Description}


\bs

\ll{Background of the problem statement:}

\ms

A popular payment application, EasyPay where users add
money to their wallet accounts, faces an issue in its
payment success rate. The timeout that occurs with the
connectivity of the database has been the reason for the issue.
While troubleshooting, it is found that the database server
has several downtime instances at irregular intervals. This
situation compels the company to create their own infrastructure
that runs in high-availability mode.  Given that online shopping
experiences continue to evolve as per customer expectations,
the developers are driven to make their app more reliable, fast,
and secure for improving the performance of the current system.


\bs

\ll{Solution}

\ms

Create a DevOps infrastructure for an e-commerce application to run on {\bf HA\/} (high availability) node.


\ll{High-Level Details}

The application will run on Amazon's Elastic Kubernetes Service (EKS). EKS is a robust commercial
solution that is reliable and easily scales.  Key to the service is the deployment of
a database, running inside a Pod, that can easily be restarted for any reason.
The data in the database must be persistent, i.e., not lost if the server goes down. So, in this solution the database pod uses
a volume tied to the hardware of a particular node, using Peristent Volumes.
Since we separate the backend hardware solution from the fron-end, the vendor who employs this solution can
decide how much to invest in network, storage, and processing for the database server.

\ll{Low-Level Details}

\myitem Setting up access.

In order to get this working, we need to install the AWS CLI, EKS, and Ansible on a Mac, PC, or Linux system.
(In my particular case I setup a Debian VM running in VirtualBox on my Macbook Pro.) With these tools in
place we are ready to launch the Kubernetes cluster on AWS.


\myitem  Setting up the cluster.

EKS requires an AWS account, which has been previously setup.  Next we launch the EKS cluster using
the EKS CLI, eksctl, as follows:

\beginlines
|eksctl create cluster \ |
|    --name evansjr \  |
|     --region us-west-2 \ |
|     --nodes 3 \ |
|     --nodes-max 5 \ |
|     --with-oidc \ |
|     --ssh-access \ |
|     --managed |
\endlines

\noindent (On average it takes Amazon about twenty minutes to setup the cluster.)

Once the Kubernetes cluster is up and running, the nodes are provisioned with Ansible.
This requires that we be able to SSH into the nodes, which EKS allows using PKI certificates.
All the ansible scripts are available at the github repository for this Project:
{ \bf http://github.com/evansjr2000/Capstone.git}, and some examples are given in the
appendix.


\ll {Setting up a Persistent Volume}

The database will reside on a specific node of the cluster.  To prevent the database pod running on a node
not containing the database we will label the node.




{
\beginlines
| |
| kubectl label node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') kiamol=ch05 |
| |
\endlines
}

\break

Here is the specification for the persistent volume, which will attach to the node with the correct label,
as defined above.

\ms


\beginlines
|apiVersion: v1                        |
|: PersistentVolume                    |
|metadata:                             |
|  name: pv01                          |
|spec:                                 |
|  capacity:                           |
|    storage: 50Gi                     |
|  accessModes:                        |
|    - ReadWriteOnce                   |
|  local:                              |
|    path: /nfs_share_dir              |
|  nodeAffinity:                       |
|    required:                         |
|      nodeSelectorTerms:              |
|        - matchExpressions:           |
|          - key: kiamol               |
|            operator: In              |
|            values:                   |
|              - ch05                  |
\endlines

\noindent And here is the manifest for the Persistent Volume Claim.

\ms

\beginlines apiVersion: v1
|kind: PersistentVolumeClaim  |
|metadata:                    |
|  name: postgres-pvc         |
|spec:                        |
|  accessModes:               |
|    - ReadWriteOnce          |
|  resources:                 |
|    requests:                |
|      storage: 40Mi          |
|  storageClassName: ""       |
\endlines

\ms

The access mode of {\bf ReadWriteOnce} limits access to this volume to one pod, in this case the database pod.
As previously mentioned the database  pod will run on a specified node of the cluster.

\noindent
Here are the manifests for the database service and deployment;

\beginlines
|apiVersion: v1                              |
|kind: Service                               |
|metadata:                                   |
|  name: todo-db                             |
|spec:                                       |
|  ports:                                    |
|    - port: 5432                            |
|      targetPort: 5432                      |
|  selector:                                 |
|    app: todo-db                            |
|  type: ClusterIP                           |
|---                                         |
|apiVersion: apps/v1                        |
|kind: Deployment                           |
|metadata:                                  |
|  name: todo-db                            |
|spec:                                      |
|  selector:                                |
|    matchLabels:                           |
|      app: todo-db                         |
|  template:                                |
|    metadata:                              |
|      labels:                              |
|        app: todo-db                       |
|    spec:                                  |
|      containers:                          |
|        - name: db                         |
|          image: postgres:11.6-alpine      |
|          env:                             |
|          - name: POSTGRES_PASSWORD_FILE   |
|            value: /secrets/postgres_password |
|          volumeMounts:                    |
|            - name: secret                 |
|              mountPath: "/secrets"        |
|            - name: data                   |
|              mountPath: /var/lib/postgresql/data |
|      volumes:                             |
|        - name: secret                     |
|          secret:                          |
|            secretName: todo-db-secret     |
|            defaultMode: 0400              |
|            items:                         |
|            - key: POSTGRES_PASSWORD       |
|              path: postgres_password      |
|        - name: data                       |
|          persistentVolumeClaim:           |
|            claimName: postgres-pvc        |
\endlines


To access the database we use a web front-end.  This particular example uses a LoadBalancer manifest and we allow it to scale, using a Horizontal Pod Autoscaler (HPA) manifest as follows:

\ms

\beginlines
|apiVersion: autoscaling/v1                 |
|kind: HorizontalPodAutoscaler              |
|metadata:                                  |
|  name: todo-web                           |
|  labels:                                  |
|    kiamol: ch19                           |
|spec:                                      |
|  scaleTargetRef:                          |
|    apiVersion: apps/v1                    |
|    kind: Deployment                       |
|    name: todo-web                         |
|  minReplicas: 1                           |
|  maxReplicas: 5                           |
|  targetCPUUtilizationPercentage: 10       |
\endlines

\ms

The target CPUUtilizationPercentage is set artifically low above in order to demonstrate autoscaling when the from end web server undergoes a large load.

The following is the manifest for the web service:

\ms

\beginlines
|apiVersion: v1         |
|kind: Service          |
|metadata:              |
|  name: todo-web       |
|spec:                  |
|  ports:               |
|    - port: 8081       |
|      targetPort: 80   |
|  selector:            |
|    app: todo-web      |
|  type: LoadBalancer   |
\endlines

The manifest for the deployment is very large and can be found in the appendix.


\ll{Testing}

\myiteminit

\myitem To verify that the database robustly restarts after dying (or being deleted) and that
the backend data is preserved (persistent) we show what is in the database before
deleting the service, then as the service will automatically start another deployment
of the database, we show that the data is still there.





























xxxxx

\ll{Implementation requirements:}

\ms

\myiteminit

\myitem Create the cluster (EC2 instances with load balancer and elastic IP in case of AWS)

Kubernetes provides the load balancer. An Elastic IP address is a static IPv4 address designed
for dynamic cloud computing. An Elastic IP address is allocated to your AWS account, and is yours until you release it.
Or maybe not.  

\noindent (See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html)


\ms
\myitem Automate the provisioning of an EC2 instance using Ansible or Chef Puppet

(I have scripts to do this.)
\ms

\myitem Install Docker and Kubernetes on the cluster
(I have scripts to do this.)
\ms

\myitem Implement the network policies at the database pod to allow ingress traffic from the front-end application pod
(Need to look this up in KIAMOL book.)
\ms

\myitem Create a new user with permissions to create, list, get, update, and delete pods
(Doable.  Must research and demonstrate how to do this.)
\ms

\myitem Configure application on the pod
(Maybe implement the database.)
\ms

\myitem Take snapshot of ETCD database
(ETCD is the key-value store for K8s.  Need to read about ``backing up K8s.''
\ms

\myitem Set criteria such that if the memory of CPU goes beyond 50%, environments automatically get scaled up and configured
(Did this as part of last class.  Need to re-implement this.)

\bs

\ll{The following tools must be used:}

\ms

\myiteminit

\myitem EC2
\myitem    Kubernetes
\myitem    Docker
\myitem     Ansible or Chef or Puppet

\bs

\ll{The following things to be kept in check:}

\ms

\myiteminit

\myitem    You need to document the steps and write the algorithms in them.
\myitem    The submission of your GitHub repository link is mandatory. In order to track your tasks, you need to share the link of the repository.
\myitem Document the step-by-step process starting from creating test cases, then executing them, and recording the results.
\myitem     You need to submit the final specification document, which includes:

\mysubitem Project and tester details
\mysubitem Concepts used in the project
\mysubitem Links to the GitHub repository to verify the project completion
\mysubitem Your conclusion on enhancing the application and defining the USPs (Unique Selling Points)


\bigskip

\ll{Appendix}


{ \eightpoint
\beginlines
| $ kubectl get nodes -o wide |
| NAME                                           STATUS   ROLES    AGE    VERSION              INTERNAL-IP      EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                CONTAINER-RUNTIME |
| ip-192-168-1-152.us-west-2.compute.internal    Ready    <none>   2d2h   v1.19.6-eks-49a6c0   192.168.1.152    34.216.36.123   Amazon Linux 2   5.4.117-58.216.amzn2.x86_64   docker://19.3.13 |
| ip-192-168-57-176.us-west-2.compute.internal   Ready    <none>   2d2h   v1.19.6-eks-49a6c0   192.168.57.176   35.80.10.134    Amazon Linux 2   5.4.117-58.216.amzn2.x86_64   docker://19.3.13 |
| ip-192-168-95-52.us-west-2.compute.internal    Ready    <none>   2d2h   v1.19.6-eks-49a6c0   192.168.95.52    34.221.188.50   Amazon Linux 2   5.4.117-58.216.amzn2.x86_64   docker://19.3.13 |
\endlines
}

Using the above information I create a {\tt hosts\/} file for Ansible to access the Kubernetes nodes:

{\ninepoint
\beginlines
|$ cat hosts | 
| [k8s-nodes] |
| k8smaster01 ansible_host=34.216.36.123 ansible_user=ec2-user |
| k8snode01 ansible_host=35.80.10.134    ansible_user=ec2-user |
| k8snode02 ansible_host=34.221.188.50   ansible_user=ec2-user |
| |
| [all:vars] |
| # Debian uses python3 instead of python2, |
| # we need to set the interpreter for ansible |
| ansible_python_interpreter=/usr/bin/python |
| |
| [kube-master] |
| k8smaster01 |
|  |
| [kube-node] |
| k8snode01 |
| k8snode02 |
\endlines
}

\ms

Here is the manifest for the deployment of the web service:

\ms

\beginlines
|apiVersion: apps/v1                           |
|kind: Deployment                              |
|metadata:                                     |
|  name: todo-web                              |
|spec:                                         |
|  selector:                                   |
|    matchLabels:                              |
|      app: todo-web                           |
|  template:                                   |
|    metadata:                                 |
|      labels:                                 |
|        app: todo-web                         |
|    spec:                                     |
|      containers:                             |
|        - name: web                           |
|          image: kiamol/ch04-todo-list        |
|          resources:                          |
|            limits:                           |
|              cpu: 250m                       |
|            requests:                         |
|              cpu: 125m                       |
|          env:                                |
|          - name: ASPNETCORE_ENVIRONMENT      |
|            value: Test                       |
|          volumeMounts:                       |
|            - name: config                    |
|              mountPath: "/app/config"        |
|              readOnly: true                  |
|            - name: secret                    |
|              mountPath: "/app/secrets"       |
|              readOnly: true                  |
|      volumes:                                |
|        - name: config                        |
|          configMap:                          |
|            name: todo-web-config             |
|            items:                            |
|            - key: config.json                |
|              path: config.json               |
|        - name: secret                        |
|          secret:                             |
|            secretName: todo-web-secret       |
|            defaultMode: 0400                 |
|            items:                            |
|            - key: secrets.json               |
|              path: secrets.json              |
\endlines

\bye


